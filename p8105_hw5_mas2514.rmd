---
title: "P8105 Homework 5"
author: "Maria Serafini"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
  )

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
  )

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

The code below creates a function that, for a fixed group size, randomly draws "birthdays" for each person, checks whether there are duplicate birthdays in the group, and returns `TRUE` or `FALSE` based on the result.

```{r}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)

  repeated_bday = length(unique(birthdays)) < n_room

  repeated_bday
  
}
```


The code below runs the `bday_sim` function 10,000 times for each group size between 2 and 50, and computes the probability that at least two people in a given group will share a birthday. 

```{r}
bday_sim_results <- 
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  ) |> 
  view()
```


This plot shows the probability calculated above as a function of group size. 

```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line()
```

From the plot we can see that as group size increases, the probability that at least two people will share a birthday increases. There is a 25% chance of at least two people sharing a birthday once the group size reaches approximately 15, a 50% chance once the group size reaches approximately 23, a 75% chance once the group size reaches approximately 32, and close to a 100% chance once the group passes approximately 50. These results indicate that a large sample size is not necessarily needed for two people to likely share a birthday. 

## Problem 2

The code below creates a function that simulates a dataset from a normal distribution with a specified mean, runs a one-sample t-test of $H_0: \mu = 0$ and returns the estimated sample mean and the p-value from the test.

```{r}
sim_power = function(mu, n_subj = 30, sigma = 5) {
  
  sim_result = t.test(rnorm(n_subj, mean = mu, sd = sigma), mu = 0)
  
  sim_result_tidy <- broom::tidy(sim_result)
  
  tibble(
      mu_hat = pull(sim_result_tidy, estimate),
      p_value = pull(sim_result_tidy, p.value))
}
```


The code below generates 5000 simulated datasets under the model 
$X \sim N(\mu, \sigma)$ with $n = 30$, $\sigma = 5$, and $\mu = 0$. For each iteration, it calls the `sim_power()` function to draw a sample and run a one-sample t-test of $H_0: \mu = 0$. The resulting tibble contains the estimated sample means and p-values for all 5000 simulated datasets.

```{r}
sim_power_results_df <-
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |>
  mutate(
    results = map(mu, sim_power)
  ) |>
  unnest(results)
```


The code below repeats the simulation for $\mu = 0{:}6$, generating 5000 datasets for each mean from the model $X \sim N(\mu, \sigma)$ with $n = 30$ and $\sigma = 5$, For every dataset, it runs a one-sample t-test of $H_0: \mu = 0$ and records the estimated mean and p-value. 

```{r}
sim_power_results_df <-
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |>
  mutate(
    results = map(mu, sim_power)
  ) |>
  unnest(results)
```


From the plot below, we can see that as the true value of μ increases (i.e., as the effect size becomes larger), the power of the test increases. When μ = 0, the proportion of rejections is close to the nominal significance level of 0.05, and as μ increases, the probability of rejecting the null hypothesis approaches 1. This illustrates that larger effect sizes are easier to detect with a one-sample t-test.

```{r}
sim_power_results_df |>
  group_by(mu) |>
  summarize(
    power = mean(p_value < 0.05),
    .groups = "drop"
  ) |> 
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True Value of μ",
    y = "Power (Probability of Rejecting False Null Hypothesis)",
    title = "Power of One-sample t-test as a Function of Effect Size"
  )
```


From the plot below, we can see that the average estimate of $\hat{\mu}$ (represented by the black line) across all samples lies very close to the 1:1 line, reflecting that the sample mean is an unbiased estimator of the true mean $\mu$. In contrast, the average of $\hat{\mu}$ among only those samples for which the null hypothesis $H_0!:;\mu = 0$ was rejected (represented by the dashed purple line) is noticeably higher than the true value of $\mu$, especially when $\mu$ is small. This occurs because significant results tend to arise only when the sample mean deviates farther from zero, creating an upward selection bias. As $\mu$ becomes large and most tests reject the null, this bias diminishes and the two averages converge.

```{r}
sim_power_results_df |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(mu_hat),                    
    avg_mu_hat_reject = mean(mu_hat[p_value < 0.05]), 
    .groups = "drop"
  ) |> 
  ggplot(aes(x = mu, y = avg_mu_hat)) +
  geom_line(color = "black") +
  geom_point(color = "black") + 
  geom_line(aes(y = avg_mu_hat_reject), color = "purple", linetype = "dashed") +
  geom_point(aes(y = avg_mu_hat_reject), color = "purple") +
  labs(
    x = "True Value of μ",
    y = "Average Estimate of μ̂",
    title = "Average Estimate of μ (All Samples & Only Samples with Rejected Null) vs. True Value of μ"
  )
```


## Problem 3

The code below imports the `homicide-data` dataset from the Washington Post. This dataset consists of individual homicide records collected from 50 large U.S. cities. Each row represents a single homicide case, and the variables provide information about the victim, the circumstances of the homicide, and the status of the investigation. Key variables include the `city` and `state` in which the homicide occurred, the `reported_date` of the homicide, the approximate geographic location of the homicide given by `longitude` and `latitude`, and `disposition` which represents the investigative status of the case. 

```{r}
homicide_df <- read_csv("data/homicide-data.csv")
```


The code below creates a `homicide_count` dataset, which includes a city-level summary of the homicide data. The `city` and `state` variables are combined into a single`city_state` variable. A misclassified city is corrected from “Tulsa, AL” to “Tulsa, OK” (confirmed via geographic coordinates provided as `latitude` and `longitude`). The data are then grouped by city, and for each city the total number of homicides and the number of unsolved homicides (those classified as “Closed without arrest” or “Open/No arrest”) are calculated.

```{r}
homicide_count_df <-
  homicide_df |> 
  unite(city_state, city, state, sep = ", ") |> 
  mutate(
    city_state = case_when(
      city_state == "Tulsa, AL" ~ "Tulsa, OK",
      TRUE ~ city_state)
    ) |>
  group_by(city_state) |> 
  summarize(
    tot_homicide = n(),
    unsolved_homicide = sum(disposition %in% c("Closed without arrest", "Open/No arrest")), 
    .groups = "drop"
  )
```


The code below uses the `prop.test()` function to estimate the proportion of homicides that are unsolved in Baltimore, MD. The results of the test are cleaned using `broom::tidy()`, and the final tibble displays the estimated proportion together with its 95% confidence interval.

```{r}
prop_unsolved_df <- 
  homicide_count_df |> 
  filter(city_state == "Baltimore, MD") |> 
  with(prop.test(unsolved_homicide, tot_homicide)) |> 
  broom::tidy()

tibble(
  estimate = pull(prop_unsolved_df, estimate),
  lower_ci = pull(prop_unsolved_df, conf.low),
  upper_ci = pull(prop_unsolved_df, conf.high)
)
```


The code below creates an `unsolved_homicide_prop` function that takes a city name and computes the proportion of homicides that are unsolved in that city using `prop.test()`, returning the estimate and its 95% confidence interval. This function is then applied to every city in the dataset using `map()`, and the results are combined into a tidy data frame. The final output, `city_prop_df`, contains the estimated unsolved homicide proportion and confidence interval for each city.

```{r}
unsolved_homicide_prop = function(data, city_state_name) {
  
prop_unsolved <- 
  data |> 
  filter(city_state == city_state_name) |> 
  with(prop.test(unsolved_homicide, tot_homicide)) |> 
  broom::tidy()

tibble(
  estimate = pull(prop_unsolved, estimate),
  lower_ci = pull(prop_unsolved, conf.low),
  upper_ci = pull(prop_unsolved, conf.high)
)
  
  
}

city_prop_df <- 
  homicide_count_df |> 
  distinct(city_state) |> 
  mutate(
    homicide_results = map(
      city_state,
      ~unsolved_homicide_prop(homicide_count_df, .x)
    )) |> 
  unnest(homicide_results)
```

